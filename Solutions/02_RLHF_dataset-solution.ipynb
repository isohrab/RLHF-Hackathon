{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3c770b6-cca0-47ad-8579-70d1d6d7e058",
   "metadata": {},
   "source": [
    "### MHP Applied science group\n",
    "# RLHF Hackathon: Reward Dataset\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"../images/dataset.png\" alt=\"Supervised Fine-tuning steps\" style=\"display: block; margin-left: auto; margin-right: auto;width:600px\">\n",
    "    <p style=\"text-align:center\">Reward Dataset, which one is better? </p>\n",
    "</div>\n",
    "\n",
    "\n",
    "A reward dataset in RLHF is essential as it provides the necessary feedback for the model to learn desired behaviors. This dataset includes examples of model outputs along with corresponding human-provided feedback, such as numerical scores or rankings, indicating the quality of the outputs. It often contains pairs of outputs with labels showing which one is better. This data format, typically in JSON, includes the input, model outputs, and their respective rewards. The reward dataset guides the model by explicitly showing human preferences, helping train the reward model to provide real-time feedback during reinforcement learning. This process optimizes the model to align with human satisfaction, ensuring it generates high-quality, relevant outputs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61442892-7a0a-4974-a830-aa0dadc25533",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "from utils import extract_answer, show_responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3554a7a-2dbf-445e-9a0c-ba40628f48fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "max_seq_length = 2048 \n",
    "dtype = None \n",
    "load_in_4bit = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05bc25a-407e-4dc5-a5dc-bda201381e99",
   "metadata": {},
   "source": [
    "### Load fine tuned model\n",
    "Now we need to load the previously fine-tuned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ab24439-4d70-42c8-abd9-ecad295c4e8f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth: Fast Mistral patching release 2024.6\n",
      "   \\\\   /|    GPU: NVIDIA A10G. Max memory: 22.191 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.3.0+cu121. CUDA = 8.6. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = TRUE. Xformers = 0.0.26.post1. FA = False.\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fdcef41b9a446ed938dd6dd171539e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"phi_3_sft_model\", # TASK 4: load the complete model with lora weights!\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")\n",
    "FastLanguageModel.for_inference(model) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e82fcb43-11a6-4758-b5b2-8513fed8382e",
   "metadata": {},
   "source": [
    "Again! we need to prepare the prompts by adding the chat tokens to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2749749-2e02-4d07-8fc7-ee3268f443d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from unsloth.chat_templates import get_chat_template\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"phi-3\", \n",
    "    mapping = {\"role\" : \"from\", \"content\" : \"value\", \"user\" : \"human\", \"assistant\" : \"gpt\"}, \n",
    ")\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    convos = examples[\"conversations\"]\n",
    "    texts = [tokenizer.apply_chat_template(convo, tokenize = False, add_generation_prompt = False) for convo in convos]\n",
    "    return { \"text\" : texts, }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e425aa3e-3ce8-49e2-9c5c-3bd6357b14b8",
   "metadata": {},
   "source": [
    "### Generate dataset\n",
    "\n",
    "Now we will iterate over the questions and generate two different responses. You will choose which one is better! Feel free to add your question!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d021f1c8-bcfc-4424-a16d-5f331596a5b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "questions = [\n",
    "    \"How do you think AI will impact cybersecurity in the next decade?\",\n",
    "    \"What do you think about the integration of quantum computing into everyday technology?\",\n",
    "    \"Which emerging technology do you believe will revolutionize the healthcare industry?\",\n",
    "    \"Hwo can I fly to mars?\"]# \"ADD your question HERE \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a6965ae0-24ef-4225-b9d6-70c04b486763",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Task: Set a proper temperature\n",
    "TEMPERATURE = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "145da5d8-ed4a-459b-89d9-ed2e799bec01",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "reward_answers = []\n",
    "for question in questions:\n",
    "    messages = [\n",
    "        {\"from\": \"human\", \"value\": question + \" give only short answer.\"},\n",
    "    ]\n",
    "    response_dict = {}\n",
    "    for i in range(2):\n",
    "        inputs = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize = True,\n",
    "            add_generation_prompt = True,\n",
    "            return_tensors = \"pt\",\n",
    "        ).to(\"cuda\")\n",
    "\n",
    "        outputs = model.generate(input_ids = inputs,\n",
    "                                 max_new_tokens = 500,\n",
    "                                 use_cache = True,\n",
    "                                temperature=TEMPERATURE,\n",
    "                                do_sample= True,)\n",
    "        answer = tokenizer.batch_decode(outputs)\n",
    "        gpt_answer = extract_answer(text=answer[0])\n",
    "        response_dict[f\"response_{i}\"] = gpt_answer\n",
    "        \n",
    "    reward_answers.append(response_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04bb60c-ab38-455c-adf5-e84497a3dde0",
   "metadata": {},
   "source": [
    "### Human feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "68e604cd-2e43-4dab-917b-9bcdf774533e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div style=\"display: flex; flex-direction: row; width: 100%;\">\n",
       "      <div style=\"width: 50%;flex: 1; padding: 10px; border: 1px solid black; margin-right: 10px;\">\n",
       "        <h3>Response 1</h3>\n",
       "          <p>\n",
       "           AI will likely enhance threat detection, automate responses, and strengthen defense mechanisms, but also introduce new vulnerabilities and ethical considerations.\n",
       "          </p>\n",
       "      </div>\n",
       "      <div style=\"width: 50%;flex: 1; padding: 10px; border: 1px solid black;\">\n",
       "        <h3>Response 2</h3>\n",
       "          <p>\n",
       "           AI will likely enhance threat detection, automate responses, and improve predictive analytics, but may also introduce new vulnerabilities.\n",
       "          </p>\n",
       "      </div>\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "which answer is better? 1 or 2?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " 1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div style=\"display: flex; flex-direction: row; width: 100%;\">\n",
       "      <div style=\"width: 50%;flex: 1; padding: 10px; border: 1px solid black; margin-right: 10px;\">\n",
       "        <h3>Response 1</h3>\n",
       "          <p>\n",
       "           Integrating quantum computing into everyday technology could revolutionize processing power and efficiency, enabling breakthroughs in fields like cryptography, materials science, and complex simulations, but it also poses challenges in terms of cost, accessibility, and security.\n",
       "          </p>\n",
       "      </div>\n",
       "      <div style=\"width: 50%;flex: 1; padding: 10px; border: 1px solid black;\">\n",
       "        <h3>Response 2</h3>\n",
       "          <p>\n",
       "           The integration of quantum computing into everyday technology has the potential to revolutionize processing power and efficiency, but it also poses significant challenges in terms of accessibility, security, and infrastructure adaptation.\n",
       "          </p>\n",
       "      </div>\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "which answer is better? 1 or 2?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " 2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div style=\"display: flex; flex-direction: row; width: 100%;\">\n",
       "      <div style=\"width: 50%;flex: 1; padding: 10px; border: 1px solid black; margin-right: 10px;\">\n",
       "        <h3>Response 1</h3>\n",
       "          <p>\n",
       "           Artificial intelligence (AI) in diagnostics and treatment personalization.\n",
       "          </p>\n",
       "      </div>\n",
       "      <div style=\"width: 50%;flex: 1; padding: 10px; border: 1px solid black;\">\n",
       "        <h3>Response 2</h3>\n",
       "          <p>\n",
       "           Artificial Intelligence (AI) in diagnostics and personalized medicine.\n",
       "          </p>\n",
       "      </div>\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "which answer is better? 1 or 2?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " 1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div style=\"display: flex; flex-direction: row; width: 100%;\">\n",
       "      <div style=\"width: 50%;flex: 1; padding: 10px; border: 1px solid black; margin-right: 10px;\">\n",
       "        <h3>Response 1</h3>\n",
       "          <p>\n",
       "           Currently, humans cannot fly to Mars; space travel is facilitated by spacecraft and technology.\n",
       "          </p>\n",
       "      </div>\n",
       "      <div style=\"width: 50%;flex: 1; padding: 10px; border: 1px solid black;\">\n",
       "        <h3>Response 2</h3>\n",
       "          <p>\n",
       "           Currently, humans cannot fly to Mars. However, space agencies are developing plans for human missions.\n",
       "          </p>\n",
       "      </div>\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "which answer is better? 1 or 2?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " 2\n"
     ]
    }
   ],
   "source": [
    "chosen = []\n",
    "for responses in reward_answers:\n",
    "  need_answer = True\n",
    "  while need_answer:\n",
    "    show_responses(responses[\"response_0\"], responses[\"response_1\"])\n",
    "    print(\"which answer is better? 1 or 2?\")\n",
    "    answer = input()\n",
    "    if answer in [\"1\", \"2\"]:\n",
    "      chosen.append(answer)\n",
    "      need_answer = False\n",
    "    else:\n",
    "      print(\"Please enter '1' or '2' as answer!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a090f1-1544-4625-93b8-f66d351077b4",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div style=\"border: 2px solid red; padding: 10px; border-radius: 5px; background-color: #ffe6e6;\">\n",
    "    <strong>Wait!</strong> If you forget to update the `TEMPERATURE`, you will get almost the same answer. ðŸ¤“\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01413c42-850c-4008-9a84-b1957a2f53f3",
   "metadata": {},
   "source": [
    "\n",
    "The temperature parameter in a language model (LLM) controls the randomness of the generated outputs. It influences the modelâ€™s prediction distribution: lower values (closer to 0) make the model more deterministic, focusing on high-probability predictions and producing more repetitive and conservative text. Higher values increase randomness, allowing the model to explore a wider range of possibilities and generate more creative or diverse outputs. Adjusting the temperature helps balance between predictability and creativity, depending on the desired outcome for the generated text. In our case, `0.5` can be a good choice. you can play around with that."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6524da-177a-4921-830f-2fac41cff2e4",
   "metadata": {},
   "source": [
    "### Save the dataset\n",
    "\n",
    "A reward dataset is usually made of a `prompt`, a `chosen` and a `rejected` column. Let's save our feedback in a json record file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "078fdf95-cda9-4c54-a8ee-1247e0d4f4fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Task: Use the proper key for the dictionary\n",
    "reward_dataset = []\n",
    "for i, responses in enumerate(reward_answers):\n",
    "    json_record = {\n",
    "        \"prompt\":questions[i], # Update Key\n",
    "    }\n",
    "    if chosen[i] == 2:\n",
    "        json_record[\"chosen\"]= responses[\"response_1\"] # Update key\n",
    "        json_record[\"rejeted\"]= responses[\"response_0\"] # Update key\n",
    "    else:\n",
    "        json_record[\"chosen\"]= responses[\"response_0\"] # Update key\n",
    "        json_record[\"rejeted\"]= responses[\"response_1\"] # Update key\n",
    "    reward_dataset.append(json_record)\n",
    "        \n",
    "\n",
    "\n",
    "assert len(reward_dataset) > 0, \"Your dataset is empty\"\n",
    "\n",
    "import json\n",
    "json.dump(reward_dataset, open(\"reward_dataset.json\", \"w\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627978df-df5c-405e-854b-35a7a38c7914",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
