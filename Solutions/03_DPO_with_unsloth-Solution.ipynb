{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3fbf5eb-d423-4003-9a37-2952ae4611bf",
   "metadata": {
    "tags": []
   },
   "source": [
    "### MHP Applied science group\n",
    "# RLHF Hackathon: DPO\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"../images/dpo.png\" alt=\"Supervised Fine-tuning steps\" style=\"display: block; margin-left: auto; margin-right: auto;width:800px\">\n",
    "    <p style=\"text-align:center\">Read more about DPO algorithm in the <a href=\"https://arxiv.org/abs/2305.18290\">original paper</a>.</p>\n",
    "</div>\n",
    "\n",
    "Direct Preference Optimization (DPO) is a technique used to fine-tune models by directly optimizing for human preferences. This method aims to align the model’s outputs more closely with what humans consider high-quality or relevant, improving the overall user experience. In order to do DPO on a LLM we need to do following steps\n",
    "\n",
    " 1. Collecting Preference Data: The first step involves gathering a dataset of human preferences. This can be done through surveys, user interactions, or expert annotations where humans rank or score different outputs of the model based on their quality or relevance.\n",
    " 2. Defining a Reward Function: A reward function is created based on the collected preference data. This function assigns scores to different outputs, indicating how well they align with human preferences. The reward function serves as the objective for optimization.\n",
    " 3. Training the Model: Using the reward function, the model is trained to generate outputs that maximize the reward. This involves adjusting the model’s parameters to produce outputs that are more likely to be preferred by humans. Techniques such as gradient ascent or reinforcement learning can be used for this optimization.\n",
    " 4. Evaluating and Refining: After training, the model’s outputs are evaluated against human preferences to ensure alignment. If necessary, the preference data can be refined, and the model can be further tuned to improve its performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1e63c4e-e1a1-47c9-8b04-29469c53b69b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4613b0b7-913b-4997-a52d-df7debc9c599",
   "metadata": {},
   "source": [
    "### Load fine tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e4ccb7c-e28c-4969-9a51-42d8755c9014",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth: Fast Mistral patching release 2024.6\n",
      "   \\\\   /|    GPU: NVIDIA A10G. Max memory: 22.191 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.3.0+cu121. CUDA = 8.6. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = TRUE. Xformers = 0.0.26.post1. FA = False.\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "max_seq_length = 4096 \n",
    "dtype = None \n",
    "load_in_4bit = True \n",
    "\n",
    "# We will start from Phi-3 model as it already fine tuned over a large number of datasets\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Phi-3-mini-4k-instruct\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405f4f06-27c6-4009-b910-76b0d5e7095d",
   "metadata": {},
   "source": [
    "### Load Dataset\n",
    "\n",
    "We need to load an RLHF dataset (or reward dataset). We don’t use our reward dataset because it has few questions. A good reward dataset should cover all the cases that we, as humans, prefer. For this purpose, we load a dataset from Huggingface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a241d78f-ede5-4e3d-b327-b791ed0478b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import random\n",
    "random.seed(711)\n",
    "sample_size = 50\n",
    "dataset = load_dataset(\"HuggingFaceH4/ultrafeedback_binarized\", split=\"train_prefs\")\n",
    "random_indices = random.sample(range(len(dataset)), sample_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6936cc0-366b-4ea4-8c89-39d4b013bbca",
   "metadata": {},
   "source": [
    "<div style=\"border: 2px solid red; padding: 10px; border-radius: 5px; background-color: #ffe6e6;\">\n",
    "    <strong>Wait!</strong> But the dataset and Chat Format are not aligned!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e5edfc4-71bb-4e95-9a60-47f999ad7116",
   "metadata": {},
   "source": [
    "In order to align the dataset with the expected input of the LLM, we need to update our dataset format. We also adapt the dataset so that `DPO` expects it for training. It got complicated, right? No worries, let's break it down.\n",
    "\n",
    "#### LLM Chat Format\n",
    "As we already saw in the previous notebook, the LLM expects text where the beginnings and ends of prompts and responses are marked with special tokens. To find out which tokens `Phi-3` uses, take a look at its [model card](https://huggingface.co/microsoft/Phi-3-mini-4k-instruct#chat-format). Find the special tokens and complete the function.\n",
    "\n",
    "#### DPO Dataset Format\n",
    "Now we need to change the records to match what the DPO trainer expects. Based on the [DPO documentation](https://huggingface.co/docs/trl/main/en/dpo_trainer), DPO expects three entries for each record as follows:\n",
    "- **prompt**: `<s>` + `<user_token>` + prompt_text + `<end_token>` + `<assistant_token>`\n",
    "- **chosen**: chosen_text + `<end_token>`\n",
    "- **rejected**: rejected_text + `<end_token>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c76ce781-7207-4e64-9c34-7ca640a99345",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TASK 5: find out chat format\n",
    "def apply_chat_template(\n",
    "    example,\n",
    "    assistant_token= \"<assistant_token>\", # UPDATE HERE,\n",
    "    user_token = \"<user_token>\", # UPDATE HERE,\n",
    "    eos_token = \"<end_token>\", # UPDATE HERE,\n",
    "    bos_token = \"<s>\",\n",
    "):\n",
    "    if all(k in example.keys() for k in (\"chosen\", \"rejected\")):\n",
    "        chosen_messages = example[\"chosen\"][1:]\n",
    "        rejected_messages = example[\"rejected\"][1:]\n",
    "        example[\"text_prompt\"] = f\"{bos_token}{user_token}\\n{example['prompt']}{eos_token}\\n{assistant_token}\"\n",
    "        example[\"text_chosen\"] = f\"{chosen_messages[0]['content']}{eos_token}\\n\"\n",
    "        example[\"text_rejected\"] = f\"{rejected_messages[0]['content']}{eos_token}\\n\"\n",
    "        \n",
    "    return example\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1c72f2-881b-4d4b-870a-b8c3ebbfc9d1",
   "metadata": {
    "tags": []
   },
   "source": [
    "We only use 100 examples. We need to convert the dataset to the desired format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b75ac2d3-b813-4c4c-a873-6e0a26dcbd3c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Select the samples\n",
    "sampled_dataset = dataset.select(random_indices)\n",
    "\n",
    "# we hold the column names to remove them later\n",
    "column_names = sampled_dataset.column_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7c4b4b-df41-4a34-9e31-ab09db39cd08",
   "metadata": {},
   "source": [
    "We need to add the Phi-3 chat format to our dataset. let's do it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bb81d0c1-3b3f-4268-ad16-c29e7e64d9e2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3163b1b34b54a81a0bc7c58bc9f365b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Task: Applay chat template to dataset\n",
    "sampled_dataset = sampled_dataset.map(apply_chat_template) # Update here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c016182-360b-49a3-91df-295af31aceea",
   "metadata": {},
   "source": [
    "Now the dataset has duplicated records. The original column does not have the tokens and should be removed. The `text_<column_name>` column has the special tokens, but the column name is not correct. We need to rename `text_<column_name>` to `<column_name>`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "de19b5a6-7c2c-4178-94fe-42a075babe48",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sampled_dataset = sampled_dataset.remove_columns(column_names)\n",
    "\n",
    "# Task: rename the columns\n",
    "sampled_dataset = sampled_dataset.rename_columns(\n",
    "        {\"text_prompt\": \"prompt\", \"text_chosen\": \"chosen\", \"text_rejected\": \"rejected\"} # Update here\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fe0d8927-68b3-46ef-aae0-b87e615849f6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "('<s><user_token>\\n'\n",
      " 'Premise: \"A man with tattoos sits on a chair in the grass.\"\\n'\n",
      " 'Based on this premise, can we conclude that the hypothesis \"A man sits on a '\n",
      " 'chair.\" is true?\\n'\n",
      " 'Options:\\n'\n",
      " '- yes\\n'\n",
      " '- it is not possible to tell\\n'\n",
      " \"- no Now, let's be accurate as possible. Some thinking first:<end_token>\\n\"\n",
      " '<assistant_token>')\n",
      "*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "('Yes, it is possible to conclude that the hypothesis \"A man sits on a chair\" '\n",
      " 'is true based on the given premise \"A man with tattoos sits on a chair in '\n",
      " 'the grass.\" So, the answer is yes. Confidence: 95%<end_token>\\n')\n",
      "*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "('Based on the premise \"A man with tattoos sits on a chair in the grass,\" we '\n",
      " 'cannot conclude that the hypothesis \"A man sits on a chair\" is true with '\n",
      " 'certainty. It is possible that the man in the premise is not sitting on a '\n",
      " 'chair, but rather is standing or lying down. Additionally, the premise '\n",
      " 'provides no information about the type of chair or where the man is sitting. '\n",
      " 'Therefore, it is not possible to tell with certainty whether the hypothesis '\n",
      " 'is true based on this premise alone.<end_token>\\n')\n"
     ]
    }
   ],
   "source": [
    "# Let’s see what we have done.\n",
    "import pprint\n",
    "row = sampled_dataset[1]\n",
    "print(\"-\".join([\"*\"]*30))\n",
    "pprint.pprint(row[\"prompt\"])\n",
    "print(\"-\".join([\"*\"]*30))\n",
    "pprint.pprint(row[\"chosen\"])\n",
    "print(\"-\".join([\"*\"]*30))\n",
    "pprint.pprint(row[\"rejected\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a41704-ddeb-468e-85cb-1244363d8ca9",
   "metadata": {},
   "source": [
    "### PEFT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64dfebc-2601-4321-b7e2-3112a5afef36",
   "metadata": {},
   "source": [
    "Here we use the unsloth PEFT model to efficiently fine-tune the model by updating a smaller subset of parameters, thereby reducing memory usage and accelerating the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ddae8322-a8ea-487a-87e4-22da6efa2709",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Already have LoRA adapters! We shall skip this step.\n"
     ]
    }
   ],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 64, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 64,\n",
    "    lora_dropout = 0,\n",
    "    bias = \"none\",    \n",
    "    use_gradient_checkpointing = \"unsloth\",\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  \n",
    "    loftq_config = None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "041943a0-6f99-435b-a4c9-dfc766dbfc93",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# One must patch the DPO Trainer first!\n",
    "from unsloth import PatchDPOTrainer\n",
    "PatchDPOTrainer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4088dbe0-fb75-4059-aac4-823bdf5383bb",
   "metadata": {},
   "source": [
    "## DPO Training\n",
    "\n",
    "Now we are ready to fine tune the model with DPO method. Here is a brief explaination of the parameters: \n",
    "\n",
    " - **model**: The pre-trained model to be fine-tuned.\n",
    " - **per_device_train_batch_size**: Number of training samples per batch for each device.\n",
    " - **gradient_accumulation_steps**: Number of steps to accumulate gradients before updating model weights.\n",
    " - **warmup_ratio**: Fraction of training steps to perform learning rate warmup.\n",
    " - **num_train_epochs**: Total number of training epochs.\n",
    " - **learning_rate**: The initial learning rate for training.\n",
    " - **fp16**: Flag to use 16-bit floating point precision if bfloat16 is not supported.\n",
    " - **bf16**: Flag to use bfloat16 precision if supported.\n",
    " - **logging_steps**: Number of steps between logging training metrics.\n",
    " - **optim**: Optimizer type, here “adamw_8bit” for memory-efficient training.\n",
    " - **weight_decay**: Weight decay coefficient for regularization.\n",
    " - **lr_scheduler_type**: Type of learning rate scheduler to use.\n",
    " - **seed**: Random seed for reproducibility.\n",
    " - **output_dir**: Directory to save the training outputs.\n",
    " - **beta**: Regularization parameter for DPO.\n",
    " - **train_dataset**: The training dataset.\n",
    " - **tokenizer**: The tokenizer for preparing input data.\n",
    " - **max_length**: Maximum length of the sequences for the model inputs.\n",
    " - **max_prompt_length**: Maximum length for the prompt part of the input sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "12d73a74-856d-47d8-b73c-e4a4d08eab6e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:332: UserWarning: When using DPODataCollatorWithPadding, you should set `remove_unused_columns=False` in your TrainingArguments we have set it for you, but you should do it yourself in the future.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb3f1f5af3e546ffb8d255775afd2b2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "from trl import DPOTrainer, PPOTrainer\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "dpo_trainer = DPOTrainer(\n",
    "    model = model,\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 4,\n",
    "        gradient_accumulation_steps = 2,\n",
    "        warmup_ratio = 0.1,\n",
    "        num_train_epochs = 5,\n",
    "        learning_rate = 5e-6,\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 5,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.0,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 42,\n",
    "        output_dir = \"outputs\",\n",
    "    ),\n",
    "    beta = 0.1,\n",
    "    train_dataset = sampled_dataset,\n",
    "    tokenizer = tokenizer,\n",
    "    max_length = 1024,\n",
    "    max_prompt_length = 512,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b1f956e5-347f-4f27-acb8-58a9e354c99a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 50 | Num Epochs = 5\n",
      "O^O/ \\_/ \\    Batch size per device = 4 | Gradient Accumulation steps = 2\n",
      "\\        /    Total batch size = 8 | Total steps = 30\n",
      " \"-____-\"     Number of trainable parameters = 119,537,664\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='30' max='30' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [30/30 04:22, Epoch 4/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>rewards / chosen</th>\n",
       "      <th>rewards / rejected</th>\n",
       "      <th>rewards / accuracies</th>\n",
       "      <th>rewards / margins</th>\n",
       "      <th>logps / rejected</th>\n",
       "      <th>logps / chosen</th>\n",
       "      <th>logits / rejected</th>\n",
       "      <th>logits / chosen</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=30, training_loss=0.6397185802459717, metrics={'train_runtime': 271.6904, 'train_samples_per_second': 0.92, 'train_steps_per_second': 0.11, 'total_flos': 0.0, 'train_loss': 0.6397185802459717, 'epoch': 4.615384615384615})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dpo_trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef3a78e-956d-428e-8a37-617efebd7d99",
   "metadata": {},
   "source": [
    "## Done!\n",
    "\n",
    "Hey, we have completed the DPO training successfully. Unlike other methods, DPO didn’t require a separate reward model, streamlining the process. Now, we can save the fine-tuned model and use it for inference. This model is optimized to generate outputs that align closely with human preferences, ensuring higher quality and more relevant results. Let’s proceed with saving the model and integrating it into our application for enhanced user experience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373888d4-2131-4208-8f66-5c0538808e0a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Optional Task: Save the model with all weights and tokenizer!\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db285c40-9de5-42b2-994b-4171dca0ff8c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Optional Task: You can ask questions (inference) from your trained model!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e351a96-e0d7-4b43-bf71-869aaa3698f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
