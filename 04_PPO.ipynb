{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e88d2373-634b-4bb3-a148-7f9f9d8cf017",
   "metadata": {},
   "source": [
    "### MHP Applied science group\n",
    "# RLHF Hackathon: PPO\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"images/PPO_process.png\" alt=\"Supervised Fine-tuning steps\" style=\"display: block; margin-left: auto; margin-right: auto;width:800px\">\n",
    "    <p style=\"text-align:center\">Read more about PPO-algorithm in the <a href=\"https://arxiv.org/abs/1707.06347\">original paper</a>.</p>\n",
    "</div>\n",
    "\n",
    "Proximal Policy Optimization (PPO) is a technique used to fine-tune models in the field of reinforcement learning. This method aims to improve the stability and efficiency of the training process by keeping policy updates within a certain range. PPO achieves this by introducing a constraint on policy changes to ensure that new policies do not deviate too far from the old policies. This results in a more stable and efficient training process, enhancing the model's performance.\n",
    "Steps to Apply PPO to an LLM\n",
    "\n",
    "The first step is to train your SFT model (Supervised Fine-tuning Trainer), to ensure the data we train on is in-distribution for the PPO algorithm. In addition we need to train a Reward model which will be used to optimize the SFT model using the PPO algorithm.\n",
    "\n",
    " 1. Rollout: The language model generates a response or continuation based on query which could be the start of a sentence.\n",
    " 2. Evaluation: The query and response are evaluated with a function, model, human feedback or some combination of them. The important thing is that this process should yield a scalar value for each query/response pair.\n",
    " 3. Optimization: This is the most complex part. In the optimisation step the query/response pairs are used to calculate the log-probabilities of the tokens in the sequences. This is done with the model that is trained and a reference model, which is usually the pre-trained model before fine-tuning. The KL-divergence between the two outputs is used as an additional reward signal to make sure the generated responses donâ€™t deviate too far from the reference language model. The active language model is then trained with PPO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13a0b11-12b5-4338-b536-34d89a6757fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b2ed37-1fc2-4dfc-a60c-fc28281cd5a6",
   "metadata": {},
   "source": [
    "### Load librarys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0939b7fa-5c8d-4e58-95f5-006a0bb0fcc2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import time\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from random import choices\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "from transformers import AutoTokenizer, pipeline\n",
    "\n",
    "from trl import PPOTrainer, PPOConfig, AutoModelForCausalLMWithValueHead, create_reference_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1477110-f90f-44c7-83c8-1ab07ba85b7a",
   "metadata": {},
   "source": [
    "### Load fine tuned Model\n",
    "\n",
    "At a high level we need to initialize the PPOTrainer with a model we wish to train. Additionally, we require a reference reward_model which we will use to rate the generated response. \n",
    "\n",
    "The PPOConfig dataclass controls all the hyperparameters and settings for the PPO algorithm and trainer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b58cd2d-6640-43f8-9bb2-7c19f919b87d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sentiment_pipe_kwargs = {\"top_k\": None, \"function_to_apply\": \"none\"}\n",
    "\n",
    "config = PPOConfig(\n",
    "    model_name=\"lvwerra/gpt2-imdb\", steps=51200, learning_rate=1.41e-5, remove_unused_columns=False,\n",
    ")\n",
    "\n",
    "txt_in_len = 5\n",
    "txt_out_len = 20\n",
    "seed = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94006b4-9e4e-4d8b-a5b1-93a5b1d33d35",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6326e4e7-8b09-4b74-9c52-93920894769c",
   "metadata": {},
   "source": [
    "Now we can initialize our model. Note that PPO also requires a reference model, but this model is generated in a later step by the `PPOTrainer` automatically. The model can be initialized as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a367ea-3590-493b-9ebc-05c89d1b4c48",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gpt2_model = AutoModelForCausalLMWithValueHead.from_pretrained(config.model_name)\n",
    "gpt2_model_ref = create_reference_model(gpt2_model)\n",
    "gpt2_tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n",
    "\n",
    "gpt2_tokenizer.pad_token = gpt2_tokenizer.eos_token\n",
    "gpt2_tokenizer.padding_side = 'left'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c0504b-379a-4c4a-9f4b-f669beb61ea8",
   "metadata": {},
   "source": [
    "### Load Dataset\n",
    "\n",
    "The PPOTrainer expects to align a generated response with a query given the rewards obtained from the Reward model. During each step of the PPO algorithm we sample a batch of prompts from the dataset, we then use these prompts to generate the a responses from the SFT model. Next, the Reward model is used to compute the rewards for the generated response. Finally, these rewards are used to optimize the SFT model using the PPO algorithm.\n",
    "\n",
    "Therefore the dataset should contain a text column which we can rename to query. Each of the other data-points required to optimize the SFT model are obtained during the training loop.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a025fbf0-dd35-4950-b583-b8dbdeac14e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "# create the dataset\n",
    "#\n",
    "dataset = load_dataset(\"imdb\", split=\"train\")\n",
    "dataset = dataset.rename_columns({\"text\": \"review\", \"label\": \"sentiment\"})\n",
    "# make sure the comments are are at least 500 and trim to 1000\n",
    "dataset = dataset.filter(lambda x: len(x[\"review\"]) > 500, batched=False)\n",
    "dataset = dataset.map(lambda x: {\"review\": x[\"review\"][:1000]}, batched=False)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db7927b-ae1e-4b4b-bdf1-77830f07e6b1",
   "metadata": {
    "tags": []
   },
   "source": [
    "Lastly, we pretokenize our dataset using the tokenizer to ensure we can efficiently generate responses during the training loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da5309f-f4da-497d-aa9e-659566a62eac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = dataset.map(\n",
    "    lambda x: {\"input_ids\": gpt2_tokenizer.encode(\" \" + x[\"review\"], return_tensors=\"pt\")[0, :txt_in_len]},\n",
    "    batched=False,\n",
    ")\n",
    "dataset = dataset.map(lambda x: {\"query\": gpt2_tokenizer.decode(x[\"input_ids\"])}, batched=False)\n",
    "dataset = dataset[:20480]\n",
    "\n",
    "from datasets import Dataset\n",
    "\n",
    "dataset = Dataset.from_dict(dataset)\n",
    "dataset.set_format(\"pytorch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac5f542-2f40-4dc2-a448-e4b66f70f878",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset[3][\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074c0c09-7eec-46be-8bfe-bdb8c4acb9b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def collator(data):\n",
    "    return dict((key, [d[key] for d in data]) for key in data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2cfd10-9bbc-4cda-ba93-4c71c6c28b7a",
   "metadata": {},
   "source": [
    "### Using and initializing the PPOtrainer\n",
    "\n",
    "As mentioned above, we are now ready to initialize the PPOTrainer using the defined config, datasets, and model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c7ce03-7dc2-47d9-a512-33bc579b0b91",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ppo_trainer = PPOTrainer(\n",
    "    config,\n",
    "    gpt2_model,\n",
    "    gpt2_model_ref,\n",
    "    gpt2_tokenizer,\n",
    "    dataset,\n",
    "    data_collator=collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29768e7-59cd-4b74-81a4-f9d0bc01edaa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if ppo_trainer.accelerator.num_processes == 1:\n",
    "    device = 0 if torch.cuda.is_available() else \"cpu\"  # to avoid a `pipeline` bug\n",
    "else:\n",
    "    device = ppo_trainer.accelerator.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e956438e-5b3c-49f8-ba18-0d28d5b4aa63",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"we are using {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0dbaa4e-56cd-4bf4-976a-69a5bf12da53",
   "metadata": {},
   "source": [
    "The reward can be generated using any function that returns a single value for a string, be it a simple rule (e.g. length of string), a metric (e.g. BLEU), or a reward model based on human preferences. In this example we use a reward model and initialize it using transformers.pipeline for ease of use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c274ffda-4900-40ba-96a3-dbfa83ed5200",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sentiment_pipe = pipeline(\"sentiment-analysis\", \"lvwerra/distilbert-imdb\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3e87dd-d4e0-4c96-a901-b9522898dfef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text = \"this movie was really bad!!\"\n",
    "output = sentiment_pipe(text, **sentiment_pipe_kwargs)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5416658-682f-4813-9f5f-93c2bd95d451",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_pipe_output(outputs):\n",
    "    positive_logits = []\n",
    "    for out in outputs:\n",
    "        for element in out:\n",
    "            if element[\"label\"] == \"POSITIVE\":\n",
    "                positive_logits.append(torch.tensor(element[\"score\"]))\n",
    "    return positive_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb96544-0095-49de-a891-98eb7dee7ea2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "output[1][\"score\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4553197-290b-455a-bd9d-c8bdd97f6a28",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ctrl_str = [\"[negative]\", \"[neutral]\", \"[positive]\"]\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # this should be handled by accelerate\n",
    "ctrl_tokens = dict((s, gpt2_tokenizer.encode(s, return_tensors=\"pt\").squeeze().to(device)) for s in ctrl_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec81e54c-014f-4c33-b2ef-5e0f24012a65",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ctrl_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227492e3-decc-4f92-adaf-2d2fdda34c9c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def pos_logit_to_reward(logit, task):\n",
    "    \"\"\"\n",
    "    Take the positive sentiment logit and scale it for the task.\n",
    "        task [negative]: reward = -logit\n",
    "        task [neutral]: reward = -2*abs(logit)+4\n",
    "        task [positive]: reward = logit\n",
    "    \"\"\"\n",
    "    for i in range(len(logit)):\n",
    "        if task[i] == \"[negative]\":\n",
    "            logit[i] = -logit[i]\n",
    "        elif task[i] == \"[neutral]\":\n",
    "            logit[i] = -2 * torch.abs(logit[i]) + 4\n",
    "        elif task[i] == \"[positive]\":\n",
    "            pass\n",
    "        else:\n",
    "            raise ValueError(\"task has to be in [0, 1, 2]!\")\n",
    "    return logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb9fbaa-ea43-4688-8fc8-3f69879137d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(ctrl_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa1c19f-e13a-4d83-926a-15e8be8842d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pos_logit_to_reward(torch.Tensor([4, 4, 4]), ctrl_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609aecb0-db2a-4684-9eae-44b1970e68b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pos_logit_to_reward(torch.Tensor([-4, -4, -4]), ctrl_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f23a95-d363-4e4c-b3a4-268a269c24c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pos_logit_to_reward(torch.Tensor([0, 0, 0]), ctrl_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d409e4-91ab-4233-9d22-3478527660eb",
   "metadata": {},
   "source": [
    "### Starting the training loop\n",
    "\n",
    "Because the PPOTrainer needs an active reward per execution step, we need to define a method to get rewards during each step of the PPO algorithm. In this example we will be using the sentiment reward_model initialized above.\n",
    "\n",
    "To guide the generation process we use the generation_kwargs which are passed to the model.generate method for the SFT-model during each step.\n",
    "\n",
    "We can then loop over all examples in the dataset and generate a response for each query. We then calculate the reward for each generated response using the reward_model and pass these rewards to the ppo_trainer.step method. The ppo_trainer.step method will then optimize the SFT model using the PPO algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f632d3-7048-4c52-9e70-3f90699475b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gpt2_tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740249d7-f71f-443e-b2ac-5f55a65c73a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gpt2_tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd974ff9-dd1e-442f-a9eb-267e170e82e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "generation_kwargs = {\n",
    "    \"min_length\": -1,\n",
    "    \"top_k\": 0.0,\n",
    "    \"top_p\": 1.0,\n",
    "    \"do_sample\": True,\n",
    "    \"pad_token_id\": gpt2_tokenizer.eos_token_id,\n",
    "    \"max_new_tokens\": txt_out_len,\n",
    "    \"eos_token_id\": gpt2_tokenizer.eos_token_id,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faba1b7e-8a2a-4144-90d8-00e3a77c03ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "max_batch = 10\n",
    "current_batch = 0\n",
    "for epoch in range(2):\n",
    "    for batch in tqdm(ppo_trainer.dataloader):\n",
    "        (logs, game_data,) = (\n",
    "            dict(),\n",
    "            dict(),\n",
    "        )\n",
    "\n",
    "        #### prepend a random control token\n",
    "        task_list = choices(ctrl_str, k=config.batch_size)\n",
    "        game_data[\"query\"] = [t + q for t, q in zip(task_list, batch[\"query\"])]\n",
    "        query_tensors = [torch.cat((ctrl_tokens[t], input_ids)) for t, input_ids in zip(task_list, batch[\"input_ids\"])]\n",
    "\n",
    "        #### get response from gpt2\n",
    "        response_tensors = []\n",
    "        for query in query_tensors:\n",
    "            response = ppo_trainer.generate(query, **generation_kwargs)\n",
    "            response_tensors.append(response.squeeze()[-txt_out_len:])\n",
    "        game_data[\"response\"] = [gpt2_tokenizer.decode(r.squeeze()) for r in response_tensors]\n",
    "\n",
    "        #### sentiment analysis\n",
    "        texts = [q + r for q, r in zip(batch[\"query\"], game_data[\"response\"])]\n",
    "        logits = extract_pipe_output(sentiment_pipe(texts, **sentiment_pipe_kwargs))\n",
    "        rewards = pos_logit_to_reward(logits, task_list)\n",
    "\n",
    "        #### Run PPO training\n",
    "        t = time.time()\n",
    "        stats = ppo_trainer.step(query_tensors, response_tensors, rewards)\n",
    "\n",
    "        for cs in ctrl_str:\n",
    "            key = \"env/reward_\" + cs.strip(\"[]\")\n",
    "            stats[key] = np.mean([r.cpu().numpy() for r, t in zip(rewards, task_list) if t == cs])\n",
    "        ppo_trainer.log_stats(stats, game_data, rewards)\n",
    "        \n",
    "        # just stop after a few batch\n",
    "        current_batch +=1\n",
    "        if current_batch > max_batch:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403c4c8f-068a-411b-a4a4-fa2d2b4ec217",
   "metadata": {},
   "source": [
    "### DONE\n",
    "\n",
    "We have completed the PPO training successfully. Now, we can save the fine-tuned model and use it for inference. This model is optimized to generate outputs that align closely with human preferences, ensuring higher quality and more relevant results. Letâ€™s proceed with saving the model and integrating it into our application for enhanced user experience."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
