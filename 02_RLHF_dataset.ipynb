{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3c770b6-cca0-47ad-8579-70d1d6d7e058",
   "metadata": {},
   "source": [
    "### MHP Applied science group\n",
    "# RLHF Hackathon: Reward Dataset\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"images/dataset.png\" alt=\"Supervised Fine-tuning steps\" style=\"display: block; margin-left: auto; margin-right: auto;width:600px\">\n",
    "    <p style=\"text-align:center\">Reward Dataset, which one is better? </p>\n",
    "</div>\n",
    "\n",
    "\n",
    "A reward dataset in RLHF is essential as it provides the necessary feedback for the model to learn desired behaviors. This dataset includes examples of model outputs along with corresponding human-provided feedback, such as numerical scores or rankings, indicating the quality of the outputs. It often contains pairs of outputs with labels showing which one is better. This data format, typically in JSON, includes the input, model outputs, and their respective rewards. The reward dataset guides the model by explicitly showing human preferences, helping train the reward model to provide real-time feedback during reinforcement learning. This process optimizes the model to align with human satisfaction, ensuring it generates high-quality, relevant outputs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61442892-7a0a-4974-a830-aa0dadc25533",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "from utils import extract_answer, show_responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3554a7a-2dbf-445e-9a0c-ba40628f48fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "max_seq_length = 2048 \n",
    "dtype = None \n",
    "load_in_4bit = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05bc25a-407e-4dc5-a5dc-bda201381e99",
   "metadata": {},
   "source": [
    "### Load fine tuned model\n",
    "Now we need to load the previously fine-tuned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab24439-4d70-42c8-abd9-ecad295c4e8f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"\", # TASK 4: load the completed model from last notebook!\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")\n",
    "FastLanguageModel.for_inference(model) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e82fcb43-11a6-4758-b5b2-8513fed8382e",
   "metadata": {},
   "source": [
    "Again! we need to prepare the prompts by adding the chat tokens to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2749749-2e02-4d07-8fc7-ee3268f443d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from unsloth.chat_templates import get_chat_template\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"phi-3\",\n",
    "    mapping = {\"role\" : \"from\", \"content\" : \"value\", \"user\" : \"human\", \"assistant\" : \"gpt\"},\n",
    ")\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    convos = examples[\"conversations\"]\n",
    "    texts = [tokenizer.apply_chat_template(convo, tokenize = False, add_generation_prompt = False) for convo in convos]\n",
    "    return { \"text\" : texts, }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e425aa3e-3ce8-49e2-9c5c-3bd6357b14b8",
   "metadata": {},
   "source": [
    "### Generate dataset\n",
    "\n",
    "Now we will iterate over the questions and generate two different responses. You will choose which one is better! Feel free to add your question!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d021f1c8-bcfc-4424-a16d-5f331596a5b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "questions = [\n",
    "    \"How do you think AI will impact cybersecurity in the next decade?\",\n",
    "    \"What do you think about the integration of quantum computing into everyday technology?\",\n",
    "    \"Which emerging technology do you believe will revolutionize the healthcare industry?\",\n",
    "    #\"\"]# \"ADD your question HERE \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6965ae0-24ef-4225-b9d6-70c04b486763",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Task: Set a proper temperature\n",
    "TEMPERATURE = 0.00001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145da5d8-ed4a-459b-89d9-ed2e799bec01",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "reward_answers = []\n",
    "for question in questions:\n",
    "    messages = [\n",
    "        {\"from\": \"human\", \"value\": question + \" give only short answer.\"},\n",
    "    ]\n",
    "    response_dict = {}\n",
    "    for i in range(2):\n",
    "        inputs = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize = True,\n",
    "            add_generation_prompt = True,\n",
    "            return_tensors = \"pt\",\n",
    "        ).to(\"cuda\")\n",
    "\n",
    "        outputs = model.generate(input_ids = inputs,\n",
    "                                 max_new_tokens = 500,\n",
    "                                 use_cache = True,\n",
    "                                temperature=TEMPERATURE,\n",
    "                                do_sample= True,)\n",
    "        answer = tokenizer.batch_decode(outputs)\n",
    "        gpt_answer = extract_answer(text=answer[0])\n",
    "        response_dict[f\"response_{i}\"] = gpt_answer\n",
    "        \n",
    "    reward_answers.append(response_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04bb60c-ab38-455c-adf5-e84497a3dde0",
   "metadata": {},
   "source": [
    "### Human feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e604cd-2e43-4dab-917b-9bcdf774533e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chosen = []\n",
    "for responses in reward_answers:\n",
    "  need_answer = True\n",
    "  while need_answer:\n",
    "    show_responses(responses[\"response_0\"], responses[\"response_1\"])\n",
    "    print(\"which answer is better? 1 or 2?\")\n",
    "    answer = input()\n",
    "    if answer in [\"1\", \"2\"]:\n",
    "      chosen.append(answer)\n",
    "      need_answer = False\n",
    "    else:\n",
    "      print(\"Please enter '1' or '2' as answer!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a090f1-1544-4625-93b8-f66d351077b4",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div style=\"border: 2px solid red; padding: 10px; border-radius: 5px; background-color: #ffe6e6;\">\n",
    "    <strong>Wait!</strong> If you forget to update the `TEMPERATURE`, you will get almost the same answer. ðŸ¤“\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01413c42-850c-4008-9a84-b1957a2f53f3",
   "metadata": {},
   "source": [
    "\n",
    "The temperature parameter in a language model (LLM) controls the randomness of the generated outputs. It influences the modelâ€™s prediction distribution: lower values (closer to 0) make the model more deterministic, focusing on high-probability predictions and producing more repetitive and conservative text. Higher values increase randomness, allowing the model to explore a wider range of possibilities and generate more creative or diverse outputs. Adjusting the temperature helps balance between predictability and creativity, depending on the desired outcome for the generated text. In our case, `0.5` can be a good choice. you can play around with that."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6524da-177a-4921-830f-2fac41cff2e4",
   "metadata": {},
   "source": [
    "### Save the dataset\n",
    "\n",
    "A reward dataset is usually made of a `prompt`, a `chosen` and a `rejected` column. Let's save our feedback in a json record file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078fdf95-cda9-4c54-a8ee-1247e0d4f4fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Task: Use the proper key for the dictionary\n",
    "reward_dataset = []\n",
    "for i, responses in enumerate(reward_answers):\n",
    "    json_record = {\n",
    "        \"\":questions[i], # Update Key\n",
    "    }\n",
    "    if chosen[i] == 2:\n",
    "        json_record[\"\"]= responses[\"response_1\"] # Update key\n",
    "        json_record[\"\"]= responses[\"response_0\"] # Update key\n",
    "    else:\n",
    "        json_record[\"\"]= responses[\"response_0\"] # Update key\n",
    "        json_record[\"\"]= responses[\"response_1\"] # Update key\n",
    "    reward_dataset.append(json_record)\n",
    "        \n",
    "\n",
    "\n",
    "assert len(reward_dataset) > 0, \"Your dataset is empty\"\n",
    "\n",
    "import json\n",
    "json.dump(reward_dataset, open(\"reward_dataset.json\", \"w\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627978df-df5c-405e-854b-35a7a38c7914",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
