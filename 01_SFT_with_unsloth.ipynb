{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a985b543",
   "metadata": {},
   "source": [
    "### MHP Applied science group\n",
    "# RLHF Hackathon: SFT\n",
    "\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"images/img1.jpg\" alt=\"Supervised Fine-tuning steps\" style=\"display: block; margin-left: auto; margin-right: auto;width:800px\">\n",
    "    <p style=\"text-align:center\">Supervised Fine-tuning steps <a href=https://cameronrwolfe.substack.com/p/understanding-and-using-supervised>(image source)</a></p>\n",
    "</div>\n",
    "\n",
    "\n",
    "Supervised fine-tuning is the process of adapting a pre-trained model to a specific task by training it further on labeled data. This involves:\n",
    "\n",
    "\n",
    "1. **Pre-trained Model**: Starting with a model pre-trained on a large, general dataset.\n",
    "2. **Labeled Data**: Using a smaller, task-specific dataset with labeled examples.\n",
    "3. **Fine-tuning**: Updating the model's weights to improve its performance on the specific task using supervised learning methods.\n",
    "\n",
    "This method enhances the model's ability to perform well on the target task by leveraging the knowledge gained during pre-training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "086c8927-6617-4ab3-accb-9c0b0199cd6a",
   "metadata": {},
   "source": [
    "### Load model\n",
    "Let's choose a pre-trained model that fits in our instance. For this purpose, we chose `unsloth/Phi-3-mini-4k-instruct`. This model is already quantized to 4-bit with bitsandbytes. You can find more information on [Hugging Face](https://huggingface.co/unsloth/Phi-3-mini-4k-instruct). \n",
    "\n",
    "Usually each model has a **Cookbook** which has valuable information about the model. For Phi-3 you can find it [here](https://github.com/microsoft/Phi-3CookBook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08d9116-c79a-4b4f-b7d7-20570241ecaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 1: Write the name of the model\n",
    "LLM_MODEL_NAME = # update here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890d5ae6-a91d-4184-bc90-487ef5dbd6e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "max_seq_length = 2048\n",
    "dtype = None\n",
    "load_in_4bit = True\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = LLM_MODEL_NAME,\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159fbed8-ee8c-43cc-b0b9-01f196570764",
   "metadata": {},
   "source": [
    "### PEFT\n",
    "Parameter-Efficient Fine-Tuning (PEFT) refers to techniques used to adapt large pre-trained models to specific tasks with minimal adjustments to the model’s parameters. The key ideas include:\n",
    "\n",
    " 1. **Efficiency**: Only a small subset of the model’s parameters are fine-tuned, reducing computational cost and memory usage.\n",
    " 2. **Methods**: Common methods include adapters, low-rank adaptation (LoRA), and prefix-tuning.\n",
    " 3. **Advantages**: PEFT methods maintain the benefits of large pre-trained models while being resource-efficient, making them suitable for applications with limited computational resources.\n",
    "\n",
    "Overall, PEFT allows effective task adaptation without the need for extensive re-training of the entire model.\n",
    "\n",
    "Let's create peft model from previously loaded model by using `get_peft_model` from `unsloth` library. Here is quick info about the parameters:\n",
    "\n",
    " - **model**: The pre-trained language model that you want to fine-tune.\n",
    " - **r**: The rank of the low-rank adaptation matrices. Suggested values are 8, 16, 32, 64, or 128. This controls the dimension of the adaptation matrices, affecting the trade-off between efficiency and performance.\n",
    " - **target_modules**: A list of module names within the model where the low-rank adaptations will be applied. These typically include projection layers like q_proj, k_proj, v_proj, and others.\n",
    " - **lora_alpha**: A scaling factor for the low-rank adaptation matrices. Higher values can lead to stronger adaptations but might require more fine-tuning.\n",
    " - **lora_dropout**: Dropout rate for the LoRA layers. Setting it to 0 is optimized, meaning no dropout will be applied, which is typically more efficient.\n",
    " - **bias**: Defines how biases are handled in the adaptation process. The “none” setting is optimized and means that biases are not adapted, reducing computational complexity.\n",
    " - **use_gradient_checkpointing**: When set to “unsloth”, it uses an optimized gradient checkpointing technique that reduces VRAM usage by 30% and allows for larger batch sizes.\n",
    " - **random_state**: Seed for random number generation to ensure reproducibility of the fine-tuning process.\n",
    " - **use_rslora**: Boolean indicating whether to use Rank Stabilized LoRA, a technique that helps maintain the stability of the low-rank adaptations during training.\n",
    " - **loftq_config**: Configuration for LoftQ (Low-Frequency Quantization), if applicable. Setting it to None means this feature is not used.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef68d55e-7a05-4487-856c-57e9ea7c214a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    # TASK 2: UPDATE HERE,\n",
    "    r = 16, \n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0, \n",
    "    bias = \"none\",\n",
    "    use_gradient_checkpointing = \"unsloth\",\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,\n",
    "    loftq_config = None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12783ec-6ffe-49ce-a295-79a40f2c339d",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "Now, we need to prepare the dataset for our model. Each model has its own chat style. There for we need to convert the prompt/response pairs to the texts that model expect!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e891e87f-0724-42f7-9781-39509879dbf6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"phi-3\", \n",
    "    mapping = {\"role\" : \"from\", \"content\" : \"value\", \"user\" : \"human\", \"assistant\" : \"gpt\"},\n",
    ")\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    convos = examples[\"conversations\"]\n",
    "    texts = [tokenizer.apply_chat_template(convo, tokenize = False, add_generation_prompt = False) for convo in convos]\n",
    "    return { \"text\" : texts, }\n",
    "\n",
    "\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"philschmid/guanaco-sharegpt-style\", split = \"train\")\n",
    "dataset = dataset.map(formatting_prompts_func, batched = True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14872ad-8026-4c75-bf32-af739705b14d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset[5][\"conversations\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13a082d-ad46-450d-b118-c9313b3cd485",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(dataset[5][\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d657f3e-6323-48f8-a9e4-53d06019c69f",
   "metadata": {},
   "source": [
    "### SFT traineing\n",
    "\n",
    "Now we have everything to fine-tune the model. We use the RTL library from Huggingface. Let’s connect all the pieces together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62652bff-d6db-461e-8841-dbc9ead63ac4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "# TASK 3: create a trainer by using the model, tokenizer and dataset that we have created earlier\n",
    "trainer = SFTTrainer(\n",
    "    model = # UPDATE HERE,\n",
    "    tokenizer = # UPDATE HERE,\n",
    "    train_dataset = # UPDATE HERE,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dataset_num_proc = 2,\n",
    "    packing = False,\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 2,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_steps = 5,\n",
    "        max_steps = 60,\n",
    "        learning_rate = 2e-4,\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9b2cde-ddd3-4149-bbcb-06228bd6f776",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd43efbb-c083-4e9a-898a-cf0bc8878a99",
   "metadata": {},
   "source": [
    "### Inference\n",
    "\n",
    "Let's chat with our fine tuned model. Feel free to ask anything!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5928d4f-9b72-4d86-837e-c9b7e735f4b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"phi-3\",\n",
    "    mapping = {\"role\" : \"from\", \"content\" : \"value\", \"user\" : \"human\", \"assistant\" : \"gpt\"},\n",
    ")\n",
    "\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "messages = [\n",
    "    {\"from\": \"human\", \"value\": \"Continue the fibonnaci sequence: 1, 1, 2, 3, 5, 8,\"},\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize = True,\n",
    "    add_generation_prompt = True,\n",
    "    return_tensors = \"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(input_ids = inputs, max_new_tokens = 64, use_cache = True)\n",
    "tokenizer.batch_decode(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e938de01-904c-487e-b631-6273a6745f1a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "messages = [\n",
    "    {\"from\": \"human\", \"value\": \"what is the x in this equation: 3x-3 = 6\"},\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize = True,\n",
    "    add_generation_prompt = True,\n",
    "    return_tensors = \"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer)\n",
    "_ = model.generate(input_ids = inputs, streamer = text_streamer, max_new_tokens = 128, use_cache = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad31627e-2e31-4456-a784-b1b87d19bffc",
   "metadata": {},
   "source": [
    "### Save model\n",
    "\n",
    "We need to save the model for further steps in RLHF. Here we saved two different models. Do you know which one we are going to use next step and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99decf26-6d46-46f2-9308-dafc5095e588",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.save_pretrained(\"phi_3_stf_lora_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d4559c-321f-4051-be0b-c9b71f86e576",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.save_pretrained_merged(\"phi_3_sft_model\", tokenizer, save_method = \"merged_16bit\",)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
