{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3fbf5eb-d423-4003-9a37-2952ae4611bf",
   "metadata": {
    "tags": []
   },
   "source": [
    "### MHP Applied science group\n",
    "# RLHF Hackathon: DPO\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"images/dpo.png\" alt=\"Supervised Fine-tuning steps\" style=\"display: block; margin-left: auto; margin-right: auto;width:800px\">\n",
    "    <p style=\"text-align:center\">Read more about DPO algorithm in the <a href=\"https://arxiv.org/abs/2305.18290\">original paper</a>.</p>\n",
    "</div>\n",
    "\n",
    "Direct Preference Optimization (DPO) is a technique used to fine-tune models by directly optimizing for human preferences. This method aims to align the model’s outputs more closely with what humans consider high-quality or relevant, improving the overall user experience. In order to do DPO on a LLM we need to do following steps\n",
    "\n",
    " 1.\tCollecting Preference Data: The first step involves gathering a dataset of human preferences. This can be done through surveys, user interactions, or expert annotations where humans rank or score different outputs of the model based on their quality or relevance.\n",
    " 2.\tDefining a Reward Function: A reward function is created based on the collected preference data. This function assigns scores to different outputs, indicating how well they align with human preferences. The reward function serves as the objective for optimization.\n",
    " 3.\tTraining the Model: Using the reward function, the model is trained to generate outputs that maximize the reward. This involves adjusting the model’s parameters to produce outputs that are more likely to be preferred by humans. Techniques such as gradient ascent or reinforcement learning can be used for this optimization.\n",
    " 4.\tEvaluating and Refining: After training, the model’s outputs are evaluated against human preferences to ensure alignment. If necessary, the preference data can be refined, and the model can be further tuned to improve its performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554f63d4-1d20-4978-ace8-5000310fe6a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from unsloth import PatchDPOTrainer\n",
    "# PatchDPOTrainer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e63c4e-e1a1-47c9-8b04-29469c53b69b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4613b0b7-913b-4997-a52d-df7debc9c599",
   "metadata": {},
   "source": [
    "### Load fine tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4ccb7c-e28c-4969-9a51-42d8755c9014",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "max_seq_length = 4096 \n",
    "dtype = None \n",
    "load_in_4bit = True \n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"phi_3_sft_model\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405f4f06-27c6-4009-b910-76b0d5e7095d",
   "metadata": {},
   "source": [
    "### Load Dataset\n",
    "\n",
    "We need to load an RLHF dataset (or reward dataset). We don’t use our reward dataset because it has few questions. A good reward dataset should cover all the cases that we, as humans, prefer. For this purpose, we load a dataset from Huggingface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a241d78f-ede5-4e3d-b327-b791ed0478b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import random\n",
    "random.seed(711)\n",
    "sample_size = 100\n",
    "dataset = load_dataset(\"HuggingFaceH4/ultrafeedback_binarized\", split=\"train_prefs\")\n",
    "random_indices = random.sample(range(len(dataset)), sample_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6936cc0-366b-4ea4-8c89-39d4b013bbca",
   "metadata": {},
   "source": [
    "<div style=\"border: 2px solid red; padding: 10px; border-radius: 5px; background-color: #ffe6e6;\">\n",
    "    <strong>Wait!</strong> But the dataset and Chat Format are not aligned!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e5edfc4-71bb-4e95-9a60-47f999ad7116",
   "metadata": {},
   "source": [
    "In order to align the dataset with the expected input of the LLM, we need to update our dataset format. We also adapt the dataset so that `DPO` expects it for training. It got complicated, right? No worries, let's break it down.\n",
    "\n",
    "#### LLM Chat Format\n",
    "As we already saw in the previous notebook, the LLM expects text where the beginnings and ends of prompts and responses are marked with special tokens. To find out which tokens `Phi-3` uses, take a look at its [model card](https://huggingface.co/microsoft/Phi-3-mini-4k-instruct#chat-format). Find the special tokens and complete the function.\n",
    "\n",
    "#### DPO Dataset Format\n",
    "Now we need to change the records to match what the DPO trainer expects. Based on the [DPO documentation](https://huggingface.co/docs/trl/main/en/dpo_trainer), DPO expects three entries for each record as follows:\n",
    "- **prompt**: `<s>` + `<user_token>` + prompt_text + `<end_token>` + `<assistant_token>`\n",
    "- **chosen**: chosen_text + `<end_token>`\n",
    "- **rejected**: rejected_text + `<end_token>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76ce781-7207-4e64-9c34-7ca640a99345",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TASK 5: find out chat format\n",
    "def apply_chat_template(\n",
    "    example,\n",
    "    assistant_token= # UPDATE HERE,\n",
    "    user_token = # UPDATE HERE,\n",
    "    eos_token = # UPDATE HERE,\n",
    "    bos_token = \"<s>\",\n",
    "):\n",
    "    if all(k in example.keys() for k in (\"chosen\", \"rejected\")):\n",
    "        # TODO: handle case where chosen/rejected also have system messages\n",
    "        chosen_messages = example[\"chosen\"][1:]\n",
    "        rejected_messages = example[\"rejected\"][1:]\n",
    "        example[\"text_prompt\"] = f\"{user_token}\\n{example['prompt']}{eos_token}\\n{assistant_token}\"\n",
    "        example[\"text_chosen\"] = f\"{chosen_messages[0]['content']}{eos_token}\\n\"\n",
    "        example[\"text_rejected\"] = f\"{rejected_messages[0]['content']}{eos_token}\\n\"\n",
    "        \n",
    "    return example\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1c72f2-881b-4d4b-870a-b8c3ebbfc9d1",
   "metadata": {
    "tags": []
   },
   "source": [
    "We only use 100 examples. We need to convert the dataset to the desired format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75ac2d3-b813-4c4c-a873-6e0a26dcbd3c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Select the samples\n",
    "sampled_dataset = dataset.select(random_indices)\n",
    "column_names = sampled_dataset.column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb81d0c1-3b3f-4268-ad16-c29e7e64d9e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sampled_dataset = sampled_dataset.map(apply_chat_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de19b5a6-7c2c-4178-94fe-42a075babe48",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sampled_dataset = sampled_dataset.remove_columns(column_names)\n",
    "sampled_dataset = sampled_dataset.rename_columns(\n",
    "        {\"text_prompt\": \"prompt\", \"text_chosen\": \"chosen\", \"text_rejected\": \"rejected\"}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0d8927-68b3-46ef-aae0-b87e615849f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Let’s see what we have done.\n",
    "import pprint\n",
    "row = sampled_dataset[1]\n",
    "print([\"*\"]*10)\n",
    "pprint.pprint(row[\"prompt\"])\n",
    "print([\"*\"]*10)\n",
    "pprint.pprint(row[\"chosen\"])\n",
    "print([\"*\"]*10)\n",
    "pprint.pprint(row[\"rejected\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a41704-ddeb-468e-85cb-1244363d8ca9",
   "metadata": {},
   "source": [
    "### PEFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddae8322-a8ea-487a-87e4-22da6efa2709",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 64, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 64,\n",
    "    lora_dropout = 0, # Currently only supports dropout = 0\n",
    "    bias = \"none\",    # Currently only supports bias = \"none\"\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041943a0-6f99-435b-a4c9-dfc766dbfc93",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# One must patch the DPO Trainer first!\n",
    "from unsloth import PatchDPOTrainer\n",
    "PatchDPOTrainer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d73a74-856d-47d8-b73c-e4a4d08eab6e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "from trl import DPOTrainer, PPOTrainer\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "dpo_trainer = DPOTrainer(\n",
    "    model = model,\n",
    "    ref_model = None,\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 4,\n",
    "        gradient_accumulation_steps = 2,\n",
    "        warmup_ratio = 0.1,\n",
    "        num_train_epochs = 5,\n",
    "        learning_rate = 5e-6,\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 5,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.0,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 42,\n",
    "        output_dir = \"outputs\",\n",
    "    ),\n",
    "    beta = 0.1,\n",
    "    train_dataset = sampled_dataset,\n",
    "    # eval_dataset = raw_datasets[\"test\"],\n",
    "    tokenizer = # TODO\n",
    "    max_length = 1024,\n",
    "    max_prompt_length = 512,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f956e5-347f-4f27-acb8-58a9e354c99a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dpo_trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef3a78e-956d-428e-8a37-617efebd7d99",
   "metadata": {},
   "source": [
    "## Done!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
